%%\VignetteIndexEntry{The rspa package}
\documentclass[a4paper, 11pt, fleqn]{article}
\usepackage[english]{babel}
\usepackage{amsmath,amssymb}
\usepackage{natbib}
\newcommand{\bs}[1]{\boldsymbol{#1}}

%\usepackage[default]{comfortaa}
\usepackage[T1]{fontenc}
\usepackage{palatino}

\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\R}{\code{R}}
\usepackage{inconsolata}

\DeclareMathOperator*{\argmin}{\arg\min}

\setlength{\parindent}{0pt}
\setlength{\parskip}{1ex}

% tussenvoegsel hack.
\DeclareRobustCommand{\VAN}[3]{#2}


\title{The \code{rspa} package for minimal record adjustment\\
  {\small Package version \Sexpr{packageVersion("rspa")}}
}
\author{Mark van der Loo}
\date{\today}




\begin{document}
<<setup, include=FALSE, cache=FALSE>>=
library(knitr)
opts_chunk$set(size='small')
@


<<include=FALSE >>=
library(rspa)
library(editrules)
@
\maketitle

\begin{abstract}
The \code{R} extension package \code{rspa} offers functionality to minimally
adjust a vector $\bs{x}$ such that it obeys the system of (in)equations
$\bs{Ax}\leq \bs{b}$. The package implements the successive projection
algorithm that was recently described by \cite{pannekoek:2012}. There are
several ways to define what ``minimal'' means here, and the package works for
fairly large systems of equations.  Thus far it has been tested on systems with
on the order of $10^5-10^6$ variables and $10^4-10^5$ restrictions where
convergence is reached in seconds.
\end{abstract}

\tableofcontents

\newpage

\section{Introduction}
In statistics one is often confronted with records or sets of estimated values
that have to obey a set of linear equations and/or inequations. Examples
include records from business surveys that have to obey accountancy rules or
production-consumption balances for national account systems.  In practice,
such data seldom obeys all restrictions leading to further inconsistencies when
the data is used as input for further analyses. One solution is to minimally
adjust the data so that all restrictions are obeyed. 

Here, with minimal adjustment we mean that a record $\bs{x}^0\in\mathbb{R}^n$
is replaced with a value $\bs{x}$ such that the objective function
\begin{equation}
d(\bs{x},\bs{x}^0) = \left[(\bs{x} - \bs{x}^0)'\bs{W}(\bs{x}-\bs{x}^0)\right]^{1/2},
\end{equation}
is minimized as a function of $\bs{x}$, subject to 
\begin{equation}
\bs{Ax} \leq \bs{b}.
\label{eq:restrictions}
\end{equation}
Here, $\bs{W}$ is a diagonal positive weight matrix. 

Note that this problem has a simple geometric interpretation. The system
$\bs{Ax}\leq \bs{b}$ describes a convex region of $\mathbb{R}^n$ (possibly of
lower dimension than $n$). The problem is to replace the vector $\bs{x}^0$
outside this region with a record $\bs{x}$ lying in it while keeping the
distance $d(\bs{x},\bs{x}^0)$ as small as possible.

The algorithm implemented in the \code{rspa} package is a special case of
the \code{successive projection algorithm} for more general convex optimization
problems. The successive projection algorithm and the spacial case considered
here was recently applied by \cite{pannekoek:2012} to adjustment problems
related to survey data. Their paper also contains a detailed discussion
of the algorithm.


The algorithm minimizes $d(\bs{x},\bs{x}^0)$ as a function of $\bs{x}$ so that
the restrictions are obeyed up to a certain {\em accuracy}.  This accuracy is
defined as the maximum absolute deviance from Eq.\ \eqref{eq:restrictions} and
will be defined more precisely in Section \ref{sect:algorithm}.


This paper is intended as an overview of the package. For a detailed
description of functions and their parameters, refer to the manual (help pages)
that included with the package.  The rest of this paper is structured as
follows. In Sections \ref{sect:simpleexample} to \ref{sect:largeproblems} we
demonstrate some of the package's core functionality. Sections
\ref{sect:algorithm} and \ref{sect:implementation} provide some details on the
inner workings of the package which may help users to interpret the results. 



\section{A simple example}
\label{sect:simpleexample}
The following example is borrowed from \cite{pannekoek:2012} and involves
profit-loss account balances from a business survey. The problem described in
the reference involves a record of eight variables $x_1\ldots x_8$ that have
to obey the following rules.
%
\begin{eqnarray*}
x_5 &=& x_1 + x_8\\
x_5 &=& x_3 + x_4\\
x_8 &=& x_6 + x_7\\
x_4 &\geq& 0.
\end{eqnarray*}
%
The first task is to define these constraints in \R{}. The \code{rspa} package
offers several ways to do this.  One option, which we will use in this problem
is to make use of the \code{editrules} package \citep{jonge:2011}.
%
<<tidy=FALSE>>=
E <- editmatrix(expression(
    x5 == x1 + x8,
    x5 == x3 + x4,
    x8 == x6 + x7,
    x4 > 0))
@
%
%
The record in the example has the following values
<<tidy=FALSE>>=
x <- c(
   x1=330,
   x2=20,
   x3=1000,
   x4=30,
   x5=950,
   x6=500,
   x7=200,
   x8=700)
@ 
%
To confirm that this vector does not meet the constraints, we call \code{violatedEdits} (also part of \code{editrules}).
<<>>=
violatedEdits(E,x,tol=1e-2)
@
This shows that $x$ violates the first two rules (indicated with \code{TRUE}), at least to within
a tolerance of $0.01$.

In the example of \cite{pannekoek:2012}, the value of $x_5$ is considered
correct. We can therefore substitute it in the set of constraints
(\code{substValue}) and remove the corresponding variable from the set of
constraints (\code{reduce}).
\label{code:substvalue}
<<>>=
E <- reduce(substValue(E,'x5',x['x5']))
@
%
Adjusting $\bs{x}$ with the \code{adjust} function from \code{rspa} so 
it meets the constraints can be done as follows.
<<>>=
(y <- adjust(E, x))
@
%
The result is an object of class \code{adjusted}, which holds the found
solution and some convergence information on the algorithm. The solution can be
accessed as \code{y\$x}.  Using \code{violatedEdits} again, we see that now all
restrictions are obeyed.
<<>>=
violatedEdits(E, y$x, tol = 1e-2)
@
The output shows that the solution obeys the restrictions to within $10^{-2}$.
We will focus on the convergence criterion in the next section but before that,
note the following properties of the \code{adjust} function.
\begin{itemize}
\item It only adjusts variables in $\bs{x}$ that occur in at least one of the restrictions.
\item If the first argument
of \code{adjust} is an \code{editmatrix} and the variables in $\bs{x}$ are
named, they will automatically be matched so the order of variables is unimportant.
\end{itemize} 
The \code{adjust} function is a generic function and it accepts constraints in
\code{matrix}, \code{sparseConstraints} or \code{editmatrix} format. For small
adjustment problems, up to say a few hundred variables and constraints, the
\code{matrix} or \code{editmatrix} format will be fine. Large problems, with
thousands (or millions) of variables and restrictions can be defined in sparse
format and in that case the adjustment problem is solved with a routine for sparse
adjustment. This is explained further in section \ref{sect:largeproblems}.


\section{Treating many records}
\label{sect:manyrecords}
To facilitate production-wise processing of many records, the function
\code{adjustRecords} can adjust all records in a \code{data.frame} to meet
the same set of rules. The output contains the adjusted records as well
as logging information.

As an example, we create a new set of rules and generate some random data.
<<echo=FALSE>>=
set.seed(1976)
@
<<tidy=FALSE>>=
F <- editmatrix(expression(
   x + y == z,
   x >= 0,
   y >= 0,
   z >=0
))
N <- 100
dat <- data.frame(
   x = runif(100),
   y = rnorm(100),
   z = rlnorm(100)
)
@
By construction, it is very unlikely that all generated data obey the rules in
\code{F}.  To adjust the data, a single call to \code{adjustRecords} is
sufficient.
<<>>=
A <- adjustRecords(F,dat)
summary(A)
@
By default, all variables in \code{dat} are adjusted to meet the rules in
\code{F}.  However, one can optionally pass an array indicating which variables
to adjust. It is also possible to pass (an \code{array} or \code{vector} of)
weights to control the relative amount of change per variable. The return value
of \code{adjustRecords} is an object of class \code{adjustedValues}. It
contains the adjusted records in \code{A\$adjusted} and a \code{data.frame}
collecting status information in \code{A\$status}.
\begin{figure}
<<>>=
plot(A)
@
\caption{Plotting an object of class \code{adjustedRecords} yields a density
plot of the accuracy (maximum absolute deviance from constraints) and objective
function value per record.}
\label{fig:adjustedRecords}
\end{figure}


The \code{R} generic \code{summary} and \code{plot} functions have been
overloaded to get a quick glance of result quality and amount of change from
the original data.  Figure \ref{fig:adjustedRecords} shows the result of
plotting \code{A} of the above example. Two plots are created. The top panel
shows a kernel density estimate of the accuracy values for each record. The
lower panel shows a kernel density estimate of the objective function value.
The actual values are shown as a ``rug plot'' under the density plots. In this
very simple example, the accuracy is exactly zero for half of the treated
records, indicating that the constraints are met exactly after adjusting.


It is well-known that kernel density estimates can extrapolate into regions
where the actual probability density equals zero. Here, we need to make sure
that the estimated probability density for accuracy or objective function
equals zero for values $<0$. This is achieved by estimating the accuracy
density under a square root transform, and transforming back for graphical
representation. For the objective function a $\log$-transform is applied.




\section{Treating large problems}
\label{sect:largeproblems}
For problems where $\bs{x}$ has many coefficients and when there are many restrictions,
the package includes an adjustment algorithm based on a sparse representation of the
restrictions. In a sparse representation, elements of the restriction matrix $\bs{A}$
that are zero are not stored in computer memory.

Such a sparse representation is held in a \code{sparseConstraints} object. 
The function \code{sparseConstraints} constructs such objects and accepts arguments in the
form of either
\begin{itemize}
\item An \code{editmatrix}
\item A \code{matrix} $\bs{A}$, a constant vector $\bs{b}$ and an integer $n_=$ to indicate
that the first $n_=$ rows of $\bs{A}$ and $\bs{b}$ represent equalities.
\item A \code{data.frame} holding row indices, column indices and non-zero
coefficients of $\bs{A}$ in its three columns, the vector $\bs{b}$ and $n_=$.
\end{itemize}
%
For large problems, the \code{data.frame} method is probably the most convenient so this will
be demonstrated below.

\newpage
Consider again the constraints in \code{E} of page \pageref{code:substvalue}.
After substituting
$x_5=950$, it reads:
<<>>=
E
@
%
To create an object of class \code{sparseConstraints} we generate a
\code{data.frame} called \code{rc} and a vector \code{b}:
<<echo=TRUE,tidy=FALSE>>=
rc <- data.frame(
   row = c( 1, 1, 2, 2, 3, 3, 3, 4),
   col = c( 1, 2, 3, 4, 2, 5, 6, 4),
  coef = c(-1,-1,-1,-1, 1,-1,-1,-1)
)
b <- c(-950, -950, 0,0)
@
Compare these indices and coefficients with the \code{editmatrix} representation above.
With the \code{sparseConstraints} function a sparse representation is generated.
<<>>=
e <- sparseConstraints(rc, b, neq=3, sorted=TRUE)
e
@
By passing the argument \code{sorted=TRUE}, we tell \code{sparseConstraints}
that the input \code{data.frame} is sorted increasingly by column number (so it
does not have to sort it again). The function detected here that row- and
column indices are ``base 1'' (the lowest value equals 1). It is also possible
to pass coefficient definitions which are base 0. Note that we did not feed
\code{sparseConstraints} any names, so it makes up some names to represent the
rules in textual form. 

The \code{sparseConstraints} object is a \emph{reference object} that holds
a pointer to an object outside of \code{R}'s memory. Therefore, objects of
class \code{sparseConstraints}
\begin{itemize}
\item cannot be copied. Copying generates a pointer to the same object.
\item cannot be saved. Only the pointer to the external object will be stored.
The external object is destroyed by \code{R}'s garbage collector when \code{R}
closes, or when the \code{sparseConstraints} object is deleted or overwritten.
\end{itemize}
In a future version we might add a export option so that such objects can
be saved as a fixed-width file, for example.

Next, we define a new vector that matches these constraints and adjust it.
<<>>=
x_sparse <- c(330, 700, 1000, 30, 500, 200) 
(adjust(e, x_sparse))
@
Which gives the expected results.

Finally, we show in Figure \ref{fig:transcript} a transcript of a case where we
treated an adjustment problem of $474\,948$ variables under $60\,675$
constraints. We use the \code{LaF} package of \cite{laan:2012} to read the
matrix $\bs{A}$ in row-column-coefficient format from a fixed-width file. Next,
the constant vector \code{b} and \code{x} are read using \R{}'s default
\code{I/O} funcitons. The \code{sparseConstraints} object is generated and
stored in \code{e}. A printout shows the number of variables and constraints.
Because of the large number of variables no rules are printed explicitly.  A
call to \code{adjust} adjusts the vector minimally and returns an
\code{adjusted} object. The solution was found in 5.66 seconds on an
\code{Intel}$^\textrm{\textregistered}$ \code{Core}\texttrademark{}
\code{i7-2677M} CPU running at 1.80GHz.


\begin{figure}
<<eval=FALSE,tidy=FALSE,highlight=FALSE>>=
library(LaF)
## Loading required package: Rcpp
library(rspa)
## Loading required package: editrules
 
# read A-matrix
laf <- laf_open_fwf(
    file = "prob2A.txt",
    column_types = c("integer","integer","double"),
    column_widths = c(10,10,4)
    )
rowcol <- laf[]
laf <- close(laf)
 
# read b-vector
b <- read.csv("prob2b.txt",header=FALSE)[,1]
 
# read x-vector
x <- read.csv("prob2x.txt",header=FALSE)[,1]
 
e <- sparseConstraints(rowcol,b,neq=length(b))
e
## Sparse numerical constraints.
##  Variables   : 474948
##  Restrictions: 60675 (printing 0)
 
y <- adjust(e, x)
y
## Object of class 'adjusted'
##  Status    : success (using 'sparse' method)
##  Accuracy  : 0.00770226
##  Objective : 47430.5
##  Iterations: 552
##  Timing (s): 5.661
## Solution (truncated at 10):
## [1]  5.4028322  3.5246546  3.7979088  1.1202164  2.5304367  0.2037056
## [7] 97.9161357  1.5714899  4.5743395 -1.1756605
@
\caption{Transcript of treating an adjustment problem with $474\,948$ variables
under $60\,675$ equality restrictions.}
\label{fig:transcript}
\end{figure}

\clearpage


\section{About the adjustment algorithm and convergence}
\label{sect:algorithm}
The \code{rspa} package implements the {\em successive projection algorithm} as
described by \cite{pannekoek:2012}. Given a vector $\bs{x}^0$ for which
$\bs{Ax}^0\not\leq \bs{b}$.  The algorithm solves the following minimization
problem
\begin{eqnarray}
\lefteqn{
\argmin_{\bs{x}} (\bs{x}-\bs{x}^0)'\bs{W}(\bs{x} - \bs{x}^0)}\nonumber\\
s.t. &\: &\nonumber\\
\bs{Ax} &\leq& \bs{b},
\end{eqnarray}
where $\bs{W}$ is a diagonal weight matrix with all weights positive. By
default, all weights are chosen equal to $1$ in the package. In words, the
algorithm finds the vector with the smallest (weighted) Euclidean distance from
the starting vector $\bs{x}^0$ that obeys the restrictions.

To define the convergence criterion, we separate the equality from inequality
restrictions and write
\begin{eqnarray}
\label{eq:equalities}
\bs{A}_=\bs{x} &=& \bs{b}_=\\
\label{eq:inequalities}
\bs{A}_\leq\bs{x} &\leq& \bs{b}_\leq.
\end{eqnarray}
We now define $\varepsilon_=$ as the maximum difference between the 
left- and right hand side of \eqref{eq:equalities} (the infinity norm, also
know as $L_\infty$ or the Chebyshev distance):
\begin{equation}
\varepsilon_= = \left\|\bs{A}_=\bs{x} - \bs{b}_=\right\|_\infty.\\
\end{equation}
We also introduce the notation 
$
\bs{d}_\leq = \bs{A}_\leq\bs{x} - \bs{b}_\leq
$
and define
\begin{equation}
\varepsilon_\leq = \left\|\tfrac{1}{2}\left(|\bs{d}_\leq| +\bs{d}_\leq\right)\right\|_\infty.
\end{equation}
This formulation ensures that $\varepsilon_\leq >0$ only when at least one inequality
is not obeyed by $\bs{x}$. The algorithm works by iteratively improving $\bs{x}^{0}$
until the convergence parameter $\max(\varepsilon_=,\varepsilon_\leq)<\varepsilon$.
In other words: the algorithm terminates when the largest deviation from any of the 
(in)equality restrictions is met within a small parameter $\varepsilon$.

In the case that the set of user-defined constraints are infeasible
(contradictory), the algorithm either diverges, resulting in
\code{NaN}-coefficients, or the algorithm oscillates without convergence until
the maximum number of iterations have been performed. Both cases are detected
and reported. For sets of constraints that are stored as an \code{editmatrix}
object, the function \code{isFeasible} of the \code{editrules} packages is able
to check whether the set of constraints are contradictory.

Below is an example where the algorithm starts oscillating. It adjusts \code{x}
for the first constraint, violating the second, adjusts it for the second constraint,
violating the first, and so on.
<<>>=
e <- editmatrix(expression( x < 0, x > 1))
isFeasible(e)
adjust(e,c(x=0.5))
@

\section{Some notes on implementation}
\label{sect:implementation}
The core algorithms are implemented as \code{C} routines (following the
\code{C99} standard) that can be called from the \code{R} environment.  The
successive projection algorithm is implemented in a dense and a sparse version.
The dense version is called by \code{adjust.matrix} and is also the default
method that is called by \code{adjust.editmatrix}.  If the optional argument
\code{method='sparse'} is passed to \code{adjust.editmatrix}, the
\code{editmatrix} object will be coerced to a \code{sparseConstraints} object
prior to adjusting.

The \code{sparseConstraints} object is represented under the hood as a \code{C}
\code{struct} that resides outside of \code{R}'s memory. It is an
\code{R\_ExternalPtr} object, packed in an \R{} \code{environment} which is put
in a \code{S3} class. Since the object is not in \R{}'s memory, there is no point
in trying to save a \code{sparseConstraints} object: only the pointer value
will be stored while the external structure will be destroyed when \code{R}
closes.


\section{Conclusion}
With the \code{R} extension package \code{rspa}, we have made the successive
projection algorithm available for \code{R} users. In this paper we
demonstrated how (lots of) small adjustments problems can conveniently be
solved using a \code{editmatrix} definition of rules while large problems can
be solved using a sparse representation of the problem.

Future work may include extending the package to allow for the weight matrix
$\bs{W}$ to be non-diagonal and the possibility to read large problems from
multiple formats.


\section{Acknowledgements}
I am greatly indebted by Guido van den Heuvel who critically reviewed the
\code{C} code and pointed out many of the finer details of the \code{C99}
standard to me. Any remaining bugs are of course of my doing. 


\bibliographystyle{chicago}
\DeclareRobustCommand{\VAN}[3]{#3}
\bibliography{rspa}

\end{document}



